{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66f64da-7156-407d-84aa-5464780c0bd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa89d20-d37b-4457-89dc-e16ee8e1b0f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/gautham/gtk_scm/test_vlm/src/erp_cust_az12/\")   # ðŸ”¹ stores inferred schema here\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")                          # infer data types\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 100)\n",
    "    .load(\"/Volumes/gautham/gtk_scm/test_vlm/src/erp_cust_az12/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6797dc2-846e-4925-87d6-c06da1b43fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_batch(df, batch_id):\n",
    "    df=df.withColumn('cid',expr(\"\"\"CASE\n",
    "\t\t\t\tWHEN cid LIKE 'NAS%' THEN SUBSTRING(cid, 4, LEN(cid)) -- Remove 'NAS' prefix if present\n",
    "\t\t\t\tELSE cid\n",
    "\t\t\tEND \"\"\"))\n",
    "    df=df.withColumn('bdate',expr(\"\"\"CASE\n",
    "                    WHEN bdate > GETDATE() THEN NULL\n",
    "                    ELSE bdate\n",
    "                END\"\"\"))\n",
    "\n",
    "    df=df.withColumn('gen',expr(\"\"\"CASE\n",
    "                    WHEN UPPER(TRIM(gen)) IN ('F', 'FEMALE') THEN 'Female'\n",
    "                    WHEN UPPER(TRIM(gen)) IN ('M', 'MALE') THEN 'Male'\n",
    "                    ELSE 'n/a'\n",
    "                END\"\"\"))\n",
    "    df=df.withColumn(\"dwh_create_date\",lit(current_timestamp()))\n",
    "\n",
    "    src_df=df.withcolumn('audit_checksum',xxhash64(concat(coalesce('bdate',lit('null')),\n",
    "                                                      coalesce('gen',lit('null'))\n",
    "                                                     )\n",
    "                                                )\n",
    "                     )\n",
    "    \n",
    "    \n",
    "    tgt_active_df=spark.sql(\"select cid,audit_checksum from gautham.gtk_scm.erp_cust_az12 where active_flag='Y\")    \n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 2: Left join source with active target on primary key\n",
    "    # ------------------------------\n",
    "    join_df = (\n",
    "        src_df.alias(\"src\")\n",
    "        .join(tgt_active_df.alias(\"tgt\"), on=\"cst_id\", how=\"left\")\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 3: Drop completely same rows (no change)\n",
    "    # ------------------------------\n",
    "    # Rows where checksum is same => unchanged\n",
    "    changed_df = join_df.filter(\n",
    "        (F.col(\"tgt.audit_checksum\").isNull()) | \n",
    "        (F.col(\"src.audit_checksum\") != F.col(\"tgt.audit_checksum\"))\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 4: Handle changed/new records\n",
    "    # ------------------------------\n",
    "    \n",
    "    # Separate new rows and changed rows\n",
    "    new_rows_df = changed_df.filter(F.col(\"tgt.cst_id\").isNull())\n",
    "    changed_existing_df = changed_df.filter(F.col(\"tgt.cst_id\").isNotNull())\n",
    "    \n",
    "    # Create the new version rows for changed records\n",
    "    new_version_rows = changed_existing_df.select(\n",
    "        \"src.*\"\n",
    "    ).withColumn(\"effective_start_date\", F.current_timestamp()) \\\n",
    "    .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_active\", F.lit(\"Y\"))\n",
    "    \n",
    "    # Old version rows need to be deactivated\n",
    "    old_version_rows = changed_existing_df.select(\"tgt.*\").withColumn(\"is_active\", F.lit(\"N\")) \\\n",
    "        .withColumn(\"effective_end_date\", F.current_timestamp())\n",
    "    \n",
    "    # Combine all three (new inserts + new version + old version)\n",
    "    final_merge_df = (\n",
    "        new_rows_df.select(\"src.*\").withColumn(\"merge_key\", F.col(\"src.primary_key\"))\n",
    "        .unionByName(\n",
    "            old_version_rows.withColumn(\"merge_key\", F.col(\"primary_key\"))\n",
    "        )\n",
    "        .unionByName(\n",
    "            new_version_rows.withColumn(\"merge_key\", F.lit(None))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 5: Perform MERGE in a single step\n",
    "    # ------------------------------\n",
    "    \n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    delta_tgt = DeltaTable.forName(spark, \"gautham.gtk_scm.erp_cust_az12\")\n",
    "    \n",
    "    (\n",
    "        delta_tgt.alias(\"tgt\")\n",
    "        .merge(\n",
    "            final_merge_df.alias(\"src\"),\n",
    "            \"tgt.cid = src.merge_key\"\n",
    "        )\n",
    "        # update old record to inactive\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"is_active\": \"'N'\",\n",
    "            \"effective_end_date\": \"current_timestamp()\"\n",
    "        })\n",
    "        # insert new or changed version\n",
    "        .whenNotMatchedInsert(values={\n",
    "            \"cid\": \"src.cid\",\n",
    "            \"bdate\": \"src.bdate\",\n",
    "            \"gen\": \"src.gen\",\n",
    "            \"dwh_create_date\": \"src.dwh_create_date\",\n",
    "            \"audit_checksum\": \"src.audit_checksum\",\n",
    "            \"is_active\": \"'Y'\",\n",
    "            \"effective_start_date\": \"current_timestamp()\",\n",
    "            \"effective_end_date\": \"NULL\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c7c9f5-a196-49cb-9ebe-86fb6722e512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df.writeStream.foreachBatch(process_batch).option(\"checkpointLocation\", \"/Volumes/gautham/gtk_scm/test_vlm/chkp/erp_cust_az12/\").trigger(availableNow=True).start().awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "erp_cust_az12",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

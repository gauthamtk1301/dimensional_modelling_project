{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc97e30-03e6-480f-83bc-1316b03b1fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c04baa4-7a80-4b44-a259-52417996da5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/gautham/gtk_scm/test_vlm/src/erp_px_cat_g1v2/\")   # ðŸ”¹ stores inferred schema here\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")                          # infer data types\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 100)\n",
    "    .load(\"/Volumes/gautham/gtk_scm/test_vlm/src/erp_px_cat_g1v2/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac70477-376c-4a64-9db7-6d7a3b22de6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_batch(df, batch_id):\n",
    "    df=df.withColumn(\"dwh_create_date\",lit(current_timestamp()))\n",
    "\n",
    "\n",
    "    src_df=df.withcolumn('audit_checksum',xxhash64(concat(coalesce('CAT',lit('null')),\n",
    "                                                      coalesce('SUBCAT',lit('null')),\n",
    "                                                      coalesce('MAINTENANCE',lit('null'))\n",
    "                                                     )\n",
    "                                                )\n",
    "                     )\n",
    " \n",
    "    \n",
    "    tgt_active_df=spark.sql(\"select ID,audit_checksum from gautham.gtk_scm.erp_px_cat_g1v2 where active_flag='Y\")   \n",
    "    \n",
    " \n",
    "    # ------------------------------\n",
    "    # Step 2: Left join source with active target on primary key\n",
    "    # ------------------------------\n",
    "    join_df = (\n",
    "        src_df.alias(\"src\")\n",
    "        .join(tgt_active_df.alias(\"tgt\"), on=\"ID\", how=\"left\")\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 3: Drop completely same rows (no change)\n",
    "    # ------------------------------\n",
    "    # Rows where checksum is same => unchanged\n",
    "    changed_df = join_df.filter(\n",
    "        (F.col(\"tgt.audit_checksum\").isNull()) | \n",
    "        (F.col(\"src.audit_checksum\") != F.col(\"tgt.audit_checksum\"))\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 4: Handle changed/new records\n",
    "    # ------------------------------\n",
    "    \n",
    "    # Separate new rows and changed rows\n",
    "    new_rows_df = changed_df.filter(F.col(\"tgt.ID\").isNull())\n",
    "    changed_existing_df = changed_df.filter(F.col(\"tgt.ID\").isNotNull())\n",
    "    \n",
    "    # Create the new version rows for changed records\n",
    "    new_version_rows = changed_existing_df.select(\n",
    "        \"src.*\"\n",
    "    ).withColumn(\"effective_start_date\", F.current_timestamp()) \\\n",
    "    .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_active\", F.lit(\"Y\"))\n",
    "    \n",
    "    # Old version rows need to be deactivated\n",
    "    old_version_rows = changed_existing_df.select(\"tgt.*\").withColumn(\"is_active\", F.lit(\"N\")) \\\n",
    "        .withColumn(\"effective_end_date\", F.current_timestamp())\n",
    "    \n",
    "    # Combine all three (new inserts + new version + old version)\n",
    "    final_merge_df = (\n",
    "        new_rows_df.select(\"src.*\").withColumn(\"merge_key\", F.col(\"src.ID\"))\n",
    "        .unionByName(\n",
    "            old_version_rows.withColumn(\"merge_key\", F.col(\"ID\"))\n",
    "        )\n",
    "        .unionByName(\n",
    "            new_version_rows.withColumn(\"merge_key\", F.lit(None))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 5: Perform MERGE in a single step\n",
    "    # ------------------------------\n",
    "    \n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    delta_tgt = DeltaTable.forName(spark, \"gautham.gtk_scm.erp_px_cat_g1v2\")\n",
    "    \n",
    "    (\n",
    "        delta_tgt.alias(\"tgt\")\n",
    "        .merge(\n",
    "            final_merge_df.alias(\"src\"),\n",
    "            \"tgt.ID = src.merge_key\"\n",
    "        )\n",
    "        # update old record to inactive\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"is_active\": \"'N'\",\n",
    "            \"effective_end_date\": \"current_timestamp()\"\n",
    "        })\n",
    "        # insert new or changed version\n",
    "        .whenNotMatchedInsert(values={\n",
    "            \"ID\": \"src.ID\",\n",
    "            \"CAT\": \"src.CAT\",\n",
    "            \"SUBCAT\": \"src.SUBCAT\",\n",
    "            \"MAINTENANCE\": \"src.MAINTENANCE\",\n",
    "            \"dwh_create_date\": \"src.dwh_create_date\",\n",
    "            \"audit_checksum\": \"src.audit_checksum\",\n",
    "            \"is_active\": \"'Y'\",\n",
    "            \"effective_start_date\": \"current_timestamp()\",\n",
    "            \"effective_end_date\": \"NULL\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ccb106-42ad-4552-9796-5453886aa2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df.writeStream.foreachBatch(process_batch).option(\"checkpointLocation\", \"/Volumes/gautham/gtk_scm/test_vlm/chkp/erp_px_cat_g1v2/\").trigger(availableNow=True).start().awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "erp_px_cat_g1v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

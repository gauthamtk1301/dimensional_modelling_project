{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1ba2da-5cd9-468c-9027-22a0f478c09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "925eb2c5-a84b-4741-92ea-81ab3a70bed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/gautham/gtk_scm/test_vlm/scm/crm_prd_info/\")   # ðŸ”¹ stores inferred schema here\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")                          # infer data types\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 100)\n",
    "    .load(\"/Volumes/gautham/gtk_scm/test_vlm/src/crm_prd_info/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef76993-04d3-4937-9bc1-db55839d30ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_batch(df, batch_id):\n",
    "    df=df.withColumn('cat_id',expr(\"\"\"REPLACE(SUBSTRING(prd_key, 1, 5), '-', '_')\"\"\"))\n",
    "\n",
    "    df=df.withColumn('prd_cost',coalesce('prd_cost',lit(0)))\n",
    "    df=df.withColumn(\"prd_line\",expr(\"\"\"CASE \n",
    "                    WHEN UPPER(TRIM(prd_line)) = 'M' THEN 'Mountain'\n",
    "                    WHEN UPPER(TRIM(prd_line)) = 'R' THEN 'Road'\n",
    "                    WHEN UPPER(TRIM(prd_line)) = 'S' THEN 'Other Sales'\n",
    "                    WHEN UPPER(TRIM(prd_line)) = 'T' THEN 'Touring'\n",
    "                    ELSE 'n/a'\n",
    "                END\"\"\"))\n",
    "\n",
    "    df=df.withColumn('prd_start_dt',expr(\"cast(prd_start_dt as date)\"))\n",
    "    window_spec = Window.partitionBy(col(\"prd_key\")).orderBy(col(\"prd_start_dt\").asc())\n",
    "    df=df.withColumn('prd_end_dt',lead('prd_start_dt').over(window_spec))\n",
    "    df=df.withColumn('prd_key',expr(\"\"\"SUBSTRING(prd_key, 7, LEN(prd_key))\"\"\"))\n",
    "    df=df.withColumn(\"dwh_create_date\",lit(current_timestamp()))\n",
    "\n",
    "    src_df=df.withcolumn('audit_checksum',xxhash64(concat(coalesce('prd_nm',lit('null')),\n",
    "                                                      coalesce('prd_cost',lit('null')),\n",
    "                                                      coalesce('prd_line',lit('null')),\n",
    "                                                      coalesce('prd_start_dt',lit('null')),\n",
    "                                                      coalesce('prd_end_dt',lit('null')),\n",
    "                                                      coalesce('cat_id'.cast(\"string\"),lit('null'))\n",
    "                                                     )\n",
    "                                                )\n",
    "                     )\n",
    "    \n",
    "    \n",
    "    tgt_active_df=spark.sql(\"select prd_id,audit_checksum from crm_prd_info where active_flag='Y'\")    \n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 2: Left join source with active target on primary key\n",
    "    # ------------------------------\n",
    "    join_df = (\n",
    "        src_df.alias(\"src\")\n",
    "        .join(tgt_active_df.alias(\"tgt\"), on=\"prd_id\", how=\"left\")\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 3: Drop completely same rows (no change)\n",
    "    # ------------------------------\n",
    "    # Rows where checksum is same => unchanged\n",
    "    changed_df = join_df.filter(\n",
    "        (F.col(\"tgt.audit_checksum\").isNull()) | \n",
    "        (F.col(\"src.audit_checksum\") != F.col(\"tgt.audit_checksum\"))\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 4: Handle changed/new records\n",
    "    # ------------------------------\n",
    "    \n",
    "    # Separate new rows and changed rows\n",
    "    new_rows_df = changed_df.filter(F.col(\"tgt.prd_id\").isNull())\n",
    "    changed_existing_df = changed_df.filter(F.col(\"tgt.prd_id\").isNotNull())\n",
    "    \n",
    "    # Create the new version rows for changed records\n",
    "    new_version_rows = changed_existing_df.select(\n",
    "        \"src.*\"\n",
    "    ).withColumn(\"effective_start_date\", F.current_timestamp()) \\\n",
    "    .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_active\", F.lit(\"Y\"))\n",
    "    \n",
    "    # Old version rows need to be deactivated\n",
    "    old_version_rows = changed_existing_df.select(\"tgt.*\").withColumn(\"is_active\", F.lit(\"N\")) \\\n",
    "        .withColumn(\"effective_end_date\", F.current_timestamp())\n",
    "    \n",
    "    # Combine all three (new inserts + new version + old version)\n",
    "    final_merge_df = (\n",
    "        new_rows_df.select(\"src.*\").withColumn(\"merge_key\", F.col(\"src.primary_key\"))\n",
    "        .unionByName(\n",
    "            old_version_rows.withColumn(\"merge_key\", F.col(\"primary_key\"))\n",
    "        )\n",
    "        .unionByName(\n",
    "            new_version_rows.withColumn(\"merge_key\", F.lit(None))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 5: Perform MERGE in a single step\n",
    "    # ------------------------------\n",
    "    \n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    delta_tgt = DeltaTable.forName(spark, \"crm_prd_info\")\n",
    "    \n",
    "    (\n",
    "        delta_tgt.alias(\"tgt\")\n",
    "        .merge(\n",
    "            final_merge_df.alias(\"src\"),\n",
    "            \"tgt.prd_id = src.merge_key\"\n",
    "        )\n",
    "        # update old record to inactive\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"is_active\": \"'N'\",\n",
    "            \"effective_end_date\": \"current_timestamp()\"\n",
    "        })\n",
    "        # insert new or changed version\n",
    "        .whenNotMatchedInsert(values={\n",
    "            \"prd_id\": \"src.cst_id\",\n",
    "            \"prd_nm\": \"src.prd_nm\",\n",
    "            \"prd_cost\": \"src.prd_cost\",\n",
    "            \"prd_line\": \"src.prd_line\",\n",
    "            \"prd_start_dt\": \"src.prd_start_dt\",\n",
    "            \"prd_end_dt\": \"src.prd_end_dt\",\n",
    "            \"cat_id\": \"src.cat_id\",\n",
    "            \"dwh_create_date\": \"src.dwh_create_date\",\n",
    "            \"audit_checksum\": \"src.audit_checksum\",\n",
    "            \"is_active\": \"'Y'\",\n",
    "            \"effective_start_date\": \"current_timestamp()\",\n",
    "            \"effective_end_date\": \"NULL\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1792ed0f-5065-4fad-b184-feabc25093a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df.writeStream.foreachBatch(process_batch).option(\"checkpointLocation\", \"/Volumes/gautham/gtk_scm/test_vlm/chkp/crm_prd_info/\").trigger(availableNow=True).start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c76120-4d6c-43c1-8325-ee93e6c3c2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select* from gautham.gtk_scm.crm_cust_info"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7439036413804500,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "crm_prd_info",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

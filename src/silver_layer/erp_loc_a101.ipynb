{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c316a4d9-dec2-4c25-9389-f058c98a734a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96efbb5c-628b-4267-9453-6c25ccf061ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/gautham/gtk_scm/test_vlm/src/erp_loc_a101/\")   # ðŸ”¹ stores inferred schema here\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")                          # infer data types\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 100)\n",
    "    .load(\"/Volumes/gautham/gtk_scm/test_vlm/src/erp_loc_a101/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d81a2a6f-fc05-4977-80a6-4e8b4afd4c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_batch(df, batch_id):\n",
    "    df=df.withColumn('cid',expr(\"\"\"REPLACE(cid, '-', '')\"\"\"))\n",
    "    df=df.withColumn('cntry',expr(\"\"\"CASE\n",
    "                    WHEN TRIM(cntry) = 'DE' THEN 'Germany'\n",
    "                    WHEN TRIM(cntry) IN ('US', 'USA') THEN 'United States'\n",
    "                    WHEN TRIM(cntry) = '' OR cntry IS NULL THEN 'n/a'\n",
    "                    ELSE TRIM(cntry)\n",
    "                END\"\"\"))\n",
    "\n",
    "    df=df.withColumn(\"dwh_create_date\",lit(current_timestamp()))\n",
    "\n",
    "    src_df=df.withcolumn('audit_checksum',xxhash64(concat(coalesce('cntry',lit('null'))\n",
    "                                                     )\n",
    "                                                )\n",
    "                     )\n",
    "    \n",
    "    \n",
    "    tgt_active_df=spark.sql(\"select cid,audit_checksum from erp_loc_a101 where active_flag='Y'\")    \n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 2: Left join source with active target on primary key\n",
    "    # ------------------------------\n",
    "    join_df = (\n",
    "        src_df.alias(\"src\")\n",
    "        .join(tgt_active_df.alias(\"tgt\"), on=\"cid\", how=\"left\")\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 3: Drop completely same rows (no change)\n",
    "    # ------------------------------\n",
    "    # Rows where checksum is same => unchanged\n",
    "    changed_df = join_df.filter(\n",
    "        (F.col(\"tgt.audit_checksum\").isNull()) | \n",
    "        (F.col(\"src.audit_checksum\") != F.col(\"tgt.audit_checksum\"))\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 4: Handle changed/new records\n",
    "    # ------------------------------\n",
    "    \n",
    "    # Separate new rows and changed rows\n",
    "    new_rows_df = changed_df.filter(F.col(\"tgt.cid\").isNull())\n",
    "    changed_existing_df = changed_df.filter(F.col(\"tgt.cid\").isNotNull())\n",
    "    \n",
    "    # Create the new version rows for changed records\n",
    "    new_version_rows = changed_existing_df.select(\n",
    "        \"src.*\"\n",
    "    ).withColumn(\"effective_start_date\", F.current_timestamp()) \\\n",
    "    .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_active\", F.lit(\"Y\"))\n",
    "    \n",
    "    # Old version rows need to be deactivated\n",
    "    old_version_rows = changed_existing_df.select(\"tgt.*\").withColumn(\"is_active\", F.lit(\"N\")) \\\n",
    "        .withColumn(\"effective_end_date\", F.current_timestamp())\n",
    "    \n",
    "    # Combine all three (new inserts + new version + old version)\n",
    "    final_merge_df = (\n",
    "        new_rows_df.select(\"src.*\").withColumn(\"merge_key\", F.col(\"src.cid\"))\n",
    "        .unionByName(\n",
    "            old_version_rows.withColumn(\"merge_key\", F.col(\"cid\"))\n",
    "        )\n",
    "        .unionByName(\n",
    "            new_version_rows.withColumn(\"merge_key\", F.lit(None))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Step 5: Perform MERGE in a single step\n",
    "    # ------------------------------\n",
    "    \n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    delta_tgt = DeltaTable.forName(spark, \"erp_loc_a101\")\n",
    "    \n",
    "    (\n",
    "        delta_tgt.alias(\"tgt\")\n",
    "        .merge(\n",
    "            final_merge_df.alias(\"src\"),\n",
    "            \"tgt.cid = src.merge_key\"\n",
    "        )\n",
    "        # update old record to inactive\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"is_active\": \"'N'\",\n",
    "            \"effective_end_date\": \"current_timestamp()\"\n",
    "        })\n",
    "        # insert new or changed version\n",
    "        .whenNotMatchedInsert(values={\n",
    "            \"cid\": \"src.cid\",\n",
    "            \"cntry\": \"src.cst_key\",\n",
    "            \"dwh_create_date\": \"src.dwh_create_date\",\n",
    "            \"audit_checksum\": \"src.audit_checksum\",\n",
    "            \"is_active\": \"'Y'\",\n",
    "            \"effective_start_date\": \"current_timestamp()\",\n",
    "            \"effective_end_date\": \"NULL\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec0f29d-a249-4afb-922d-6db6723dbd0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df.writeStream.foreachBatch(process_batch).option(\"checkpointLocation\", \"/Volumes/gautham/gtk_scm/test_vlm/chkp/erp_loc_a101/\").trigger(availableNow=True).start().awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "erp_loc_a101",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
